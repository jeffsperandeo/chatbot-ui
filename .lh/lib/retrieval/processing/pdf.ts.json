{
    "sourceFile": "lib/retrieval/processing/pdf.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 1,
            "patches": [
                {
                    "date": 1718048452826,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718048472198,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,8 @@\n import { FileItemChunk } from \"@/types\"\n import { encode } from \"gpt-tokenizer\"\n-const { PDFLoader } = await import(\"langchain/document_loaders/fs/pdf\")\n-const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\")\n-\n+const PDFLoader = require(\"langchain/document_loaders/fs/pdf\").PDFLoader\n+const RecursiveCharacterTextSplitter = require(\"langchain/text_splitter\").RecursiveCharacterTextSplitter\n import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n \n export const processPdf = async (pdf: Blob): Promise<FileItemChunk[]> => {\n   const loader = new PDFLoader(pdf)\n"
                }
            ],
            "date": 1718048452826,
            "name": "Commit-0",
            "content": "import { FileItemChunk } from \"@/types\"\nimport { encode } from \"gpt-tokenizer\"\nconst { PDFLoader } = await import(\"langchain/document_loaders/fs/pdf\")\nconst { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\")\n\nimport { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n\nexport const processPdf = async (pdf: Blob): Promise<FileItemChunk[]> => {\n  const loader = new PDFLoader(pdf)\n  const docs = await loader.load()\n  let completeText = docs.map(doc => doc.pageContent).join(\" \")\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: CHUNK_SIZE,\n    chunkOverlap: CHUNK_OVERLAP\n  })\n  const splitDocs = await splitter.createDocuments([completeText])\n\n  let chunks: FileItemChunk[] = []\n\n  for (let i = 0; i < splitDocs.length; i++) {\n    const doc = splitDocs[i]\n\n    chunks.push({\n      content: doc.pageContent,\n      tokens: encode(doc.pageContent).length\n    })\n  }\n\n  return chunks\n}\n"
        }
    ]
}