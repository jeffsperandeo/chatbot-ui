{
    "sourceFile": "lib/retrieval/processing/txt.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 3,
            "patches": [
                {
                    "date": 1718048534330,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718048546674,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import { FileItemChunk } from \"@/types\"\n import { encode } from \"gpt-tokenizer\"\n-const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\")\n+import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"\n import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n \n export const processTxt = async (txt: Blob): Promise<FileItemChunk[]> => {\n   const fileBuffer = Buffer.from(await txt.arrayBuffer())\n"
                },
                {
                    "date": 1718048560622,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import { FileItemChunk } from \"@/types\"\n import { encode } from \"gpt-tokenizer\"\n-import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"\n+const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n \n export const processTxt = async (txt: Blob): Promise<FileItemChunk[]> => {\n   const fileBuffer = Buffer.from(await txt.arrayBuffer())\n"
                },
                {
                    "date": 1718048574581,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n import { FileItemChunk } from \"@/types\"\n import { encode } from \"gpt-tokenizer\"\n-const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n+const { RecursiveCharacterTextSplitter } = require(\"langchain/text_splitter\");\n import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n \n export const processTxt = async (txt: Blob): Promise<FileItemChunk[]> => {\n   const fileBuffer = Buffer.from(await txt.arrayBuffer())\n"
                }
            ],
            "date": 1718048534330,
            "name": "Commit-0",
            "content": "import { FileItemChunk } from \"@/types\"\nimport { encode } from \"gpt-tokenizer\"\nconst { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\")\nimport { CHUNK_OVERLAP, CHUNK_SIZE } from \".\"\n\nexport const processTxt = async (txt: Blob): Promise<FileItemChunk[]> => {\n  const fileBuffer = Buffer.from(await txt.arrayBuffer())\n  const textDecoder = new TextDecoder(\"utf-8\")\n  const textContent = textDecoder.decode(fileBuffer)\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: CHUNK_SIZE,\n    chunkOverlap: CHUNK_OVERLAP\n  })\n  const splitDocs = await splitter.createDocuments([textContent])\n\n  let chunks: FileItemChunk[] = []\n\n  for (let i = 0; i < splitDocs.length; i++) {\n    const doc = splitDocs[i]\n\n    chunks.push({\n      content: doc.pageContent,\n      tokens: encode(doc.pageContent).length\n    })\n  }\n\n  return chunks\n}\n"
        }
    ]
}