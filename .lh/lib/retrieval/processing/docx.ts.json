{
    "sourceFile": "lib/retrieval/processing/docx.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 5,
            "patches": [
                {
                    "date": 1718046421992,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718046970239,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n export const processDocX = async (text: string): Promise<FileItemChunk[]> => {\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  import { FileItemChunk } from \"@/types\";\n-  import { encode } from \"gpt-tokenizer\";\n-  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n+  const { FileItemChunk } = await import(\"@/types\");\n+  const { encode } = await import(\"gpt-tokenizer\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n \n   const splitter = new RecursiveCharacterTextSplitter({\n     chunkSize: CHUNK_SIZE,\n     chunkOverlap: CHUNK_OVERLAP\n"
                },
                {
                    "date": 1718046979342,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,7 @@\n-export const processDocX = async (text: string): Promise<FileItemChunk[]> => {\n+export const processDocX = async (text: string): Promise<{ content: string, tokens: number }[]> => {\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { FileItemChunk } = await import(\"@/types\");\n+  const FileItemChunk = await import(\"@/types\").then(module => module.FileItemChunk);\n   const { encode } = await import(\"gpt-tokenizer\");\n   const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n \n   const splitter = new RecursiveCharacterTextSplitter({\n@@ -9,9 +9,9 @@\n     chunkOverlap: CHUNK_OVERLAP\n   });\n   const splitDocs = await splitter.createDocuments([text]);\n \n-  let chunks: FileItemChunk[] = [];\n+  let chunks: { content: string, tokens: number }[] = [];\n \n   for (let i = 0; i < splitDocs.length; i++) {\n     const doc = splitDocs[i];\n \n"
                },
                {
                    "date": 1718046994967,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n export const processDocX = async (text: string): Promise<{ content: string, tokens: number }[]> => {\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const FileItemChunk = await import(\"@/types\").then(module => module.FileItemChunk);\n   const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./constants\");\n \n   const splitter = new RecursiveCharacterTextSplitter({\n     chunkSize: CHUNK_SIZE,\n     chunkOverlap: CHUNK_OVERLAP\n"
                },
                {
                    "date": 1718047005177,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,6 @@\n export const processDocX = async (text: string): Promise<{ content: string, tokens: number }[]> => {\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const FileItemChunk = await import(\"@/types\").then(module => module.FileItemChunk);\n   const { encode } = await import(\"gpt-tokenizer\");\n   const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./constants\");\n \n   const splitter = new RecursiveCharacterTextSplitter({\n"
                },
                {
                    "date": 1718050656296,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,24 +1,29 @@\n-export const processDocX = async (text: string): Promise<{ content: string, tokens: number }[]> => {\n-  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./constants\");\n+export const processDocX = async (\n+  text: string\n+): Promise<{ content: string; tokens: number }[]> => {\n+  const { RecursiveCharacterTextSplitter } = await import(\n+    \"langchain/text_splitter\"\n+  )\n+  const { encode } = await import(\"gpt-tokenizer\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./constants.ts\");\n \n+\n   const splitter = new RecursiveCharacterTextSplitter({\n     chunkSize: CHUNK_SIZE,\n     chunkOverlap: CHUNK_OVERLAP\n-  });\n-  const splitDocs = await splitter.createDocuments([text]);\n+  })\n+  const splitDocs = await splitter.createDocuments([text])\n \n-  let chunks: { content: string, tokens: number }[] = [];\n+  let chunks: { content: string; tokens: number }[] = []\n \n   for (let i = 0; i < splitDocs.length; i++) {\n-    const doc = splitDocs[i];\n+    const doc = splitDocs[i]\n \n     chunks.push({\n       content: doc.pageContent,\n       tokens: encode(doc.pageContent).length\n-    });\n+    })\n   }\n \n-  return chunks;\n-};\n+  return chunks\n+}\n"
                }
            ],
            "date": 1718046421992,
            "name": "Commit-0",
            "content": "export const processDocX = async (text: string): Promise<FileItemChunk[]> => {\n  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n  import { FileItemChunk } from \"@/types\";\n  import { encode } from \"gpt-tokenizer\";\n  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: CHUNK_SIZE,\n    chunkOverlap: CHUNK_OVERLAP\n  });\n  const splitDocs = await splitter.createDocuments([text]);\n\n  let chunks: FileItemChunk[] = [];\n\n  for (let i = 0; i < splitDocs.length; i++) {\n    const doc = splitDocs[i];\n\n    chunks.push({\n      content: doc.pageContent,\n      tokens: encode(doc.pageContent).length\n    });\n  }\n\n  return chunks;\n};\n"
        }
    ]
}