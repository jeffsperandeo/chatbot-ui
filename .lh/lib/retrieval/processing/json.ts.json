{
    "sourceFile": "lib/retrieval/processing/json.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 7,
            "patches": [
                {
                    "date": 1718046437870,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718047014940,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,10 @@\n export const processJSON = async (json: Blob): Promise<FileItemChunk[]> => {\n   const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  import { FileItemChunk } from \"@/types\";\n-  import { encode } from \"gpt-tokenizer\";\n-  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n+  const { FileItemChunk } = await import(\"@/types\");\n+  const { encode } = await import(\"gpt-tokenizer\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n \n   const loader = new JSONLoader(json);\n   const docs = await loader.load();\n   let completeText = docs.map(doc => doc.pageContent).join(\" \");\n"
                },
                {
                    "date": 1718047023783,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,29 @@\n+export const processJSON = async (json: Blob): Promise<{ content: string; tokens: number }[]> => {\n+  const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\");\n+  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n+  const { encode } = await import(\"gpt-tokenizer\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n+\n+  const loader = new JSONLoader(json);\n+  const docs = await loader.load();\n+  let completeText = docs.map(doc => doc.pageContent).join(\" \");\n+\n+  const splitter = new RecursiveCharacterTextSplitter({\n+    chunkSize: CHUNK_SIZE,\n+    chunkOverlap: CHUNK_OVERLAP\n+  });\n+  const splitDocs = await splitter.createDocuments([completeText]);\n+\n+  let chunks: { content: string; tokens: number }[] = [];\n+\n+  for (let i = 0; i < splitDocs.length; i++) {\n+    const doc = splitDocs[i];\n+\n+    chunks.push({\n+      content: doc.pageContent,\n+      tokens: encode(doc.pageContent).length\n+    });\n+  }\n+\n+  return chunks;\n+};\n"
                },
                {
                    "date": 1718049764310,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,63 +1,34 @@\n-export const processJSON = async (json: Blob): Promise<{ content: string; tokens: number }[]> => {\n-  const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\");\n-  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n+export const processJSON = async (\n+  json: Blob\n+): Promise<{ content: string; tokens: number }[]> => {\n+  const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\")\n+  const { RecursiveCharacterTextSplitter } = await import(\n+    \"langchain/text_splitter\"\n+  )\n+  const { encode } = await import(\"gpt-tokenizer\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index.ts\");\n \n-  const loader = new JSONLoader(json);\n-  const docs = await loader.load();\n-  let completeText = docs.map(doc => doc.pageContent).join(\" \");\n \n-  const splitter = new RecursiveCharacterTextSplitter({\n-    chunkSize: CHUNK_SIZE,\n-    chunkOverlap: CHUNK_OVERLAP\n-  });\n-  const splitDocs = await splitter.createDocuments([completeText]);\n+  const loader = new JSONLoader(json)\n+  const docs = await loader.load()\n+  let completeText = docs.map(doc => doc.pageContent).join(\" \")\n \n-  let chunks: { content: string; tokens: number }[] = [];\n-\n-  for (let i = 0; i < splitDocs.length; i++) {\n-    const doc = splitDocs[i];\n-\n-    chunks.push({\n-      content: doc.pageContent,\n-      tokens: encode(doc.pageContent).length\n-    });\n-  }\n-\n-  return chunks;\n-};\n-export const processJSON = async (json: Blob): Promise<FileItemChunk[]> => {\n-  const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\");\n-  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { FileItemChunk } = await import(\"@/types\");\n-  const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n-\n-  const loader = new JSONLoader(json);\n-  const docs = await loader.load();\n-  let completeText = docs.map(doc => doc.pageContent).join(\" \");\n-\n   const splitter = new RecursiveCharacterTextSplitter({\n     chunkSize: CHUNK_SIZE,\n     chunkOverlap: CHUNK_OVERLAP\n-  });\n-  const splitDocs = await splitter.createDocuments([completeText]);\n+  })\n+  const splitDocs = await splitter.createDocuments([completeText])\n \n-  let chunks: FileItemChunk[] = [];\n+  let chunks: { content: string; tokens: number }[] = []\n \n-  splitDocs.forEach(doc => {\n-    const docTokens = encode(doc.pageContent).length;\n-  });\n-\n   for (let i = 0; i < splitDocs.length; i++) {\n-    const doc = splitDocs[i];\n+    const doc = splitDocs[i]\n \n     chunks.push({\n       content: doc.pageContent,\n       tokens: encode(doc.pageContent).length\n-    });\n+    })\n   }\n \n-  return chunks;\n-};\n+  return chunks\n+}\n"
                },
                {
                    "date": 1718049780212,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,11 +5,10 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index.ts\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index\")\n \n-\n   const loader = new JSONLoader(json)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\" \")\n \n"
                },
                {
                    "date": 1718049871232,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"/Users/jeffsperandeo/Projects/chatbot-ui-fed/lib/retrieval/processing/index.ts\")\n \n   const loader = new JSONLoader(json)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\" \")\n"
                },
                {
                    "date": 1718049885577,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"/Users/jeffsperandeo/Projects/chatbot-ui-fed/lib/retrieval/processing/index.ts\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index.ts\")\n \n   const loader = new JSONLoader(json)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\" \")\n"
                },
                {
                    "date": 1718050675994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,10 +5,11 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./index.ts\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n \n+\n   const loader = new JSONLoader(json)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\" \")\n \n"
                }
            ],
            "date": 1718046437870,
            "name": "Commit-0",
            "content": "export const processJSON = async (json: Blob): Promise<FileItemChunk[]> => {\n  const { JSONLoader } = await import(\"langchain/document_loaders/fs/json\");\n  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n  import { FileItemChunk } from \"@/types\";\n  import { encode } from \"gpt-tokenizer\";\n  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n\n  const loader = new JSONLoader(json);\n  const docs = await loader.load();\n  let completeText = docs.map(doc => doc.pageContent).join(\" \");\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: CHUNK_SIZE,\n    chunkOverlap: CHUNK_OVERLAP\n  });\n  const splitDocs = await splitter.createDocuments([completeText]);\n\n  let chunks: FileItemChunk[] = [];\n\n  splitDocs.forEach(doc => {\n    const docTokens = encode(doc.pageContent).length;\n  });\n\n  for (let i = 0; i < splitDocs.length; i++) {\n    const doc = splitDocs[i];\n\n    chunks.push({\n      content: doc.pageContent,\n      tokens: encode(doc.pageContent).length\n    });\n  }\n\n  return chunks;\n};\n"
        }
    ]
}