{
    "sourceFile": "lib/retrieval/processing/csv.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 16,
            "patches": [
                {
                    "date": 1718046405250,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1718046935881,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,10 @@\n export const processCSV = async (csv: Blob): Promise<FileItemChunk[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  import { FileItemChunk } from \"@/types\";\n-  import { encode } from \"gpt-tokenizer\";\n-  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n+  const { FileItemChunk } = await import(\"@/types\");\n+  const { encode } = await import(\"gpt-tokenizer\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n \n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n"
                },
                {
                    "date": 1718046947053,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,7 @@\n-export const processCSV = async (csv: Blob): Promise<FileItemChunk[]> => {\n+export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { FileItemChunk } = await import(\"@/types\");\n   const { encode } = await import(\"gpt-tokenizer\");\n   const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n \n   const loader = new CSVLoader(csv);\n@@ -15,9 +14,9 @@\n     separators: [\"\\n\\n\"]\n   });\n   const splitDocs = await splitter.createDocuments([completeText]);\n \n-  let chunks: FileItemChunk[] = [];\n+  let chunks: { content: string; tokens: number }[] = [];\n \n   for (let i = 0; i < splitDocs.length; i++) {\n     const doc = splitDocs[i];\n \n"
                },
                {
                    "date": 1718046958273,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\".\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config\");\n \n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n"
                },
                {
                    "date": 1718047294149,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,10 +1,11 @@\n export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n \n+\n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n \n"
                },
                {
                    "date": 1718047300136,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n+const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n \n \n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n"
                },
                {
                    "date": 1718047336724,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const { encode } = await import(\"gpt-tokenizer\");\n-const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n \n \n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n"
                },
                {
                    "date": 1718047351875,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,10 @@\n export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n   const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n   const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n   const { encode } = await import(\"gpt-tokenizer\");\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.js\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n \n-\n   const loader = new CSVLoader(csv);\n   const docs = await loader.load();\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n \n"
                },
                {
                    "date": 1718049550708,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,30 +1,35 @@\n-export const processCSV = async (csv: Blob): Promise<{ content: string; tokens: number }[]> => {\n-  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n-  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n-  const { encode } = await import(\"gpt-tokenizer\");\n+export const processCSV = async (\n+  csv: Blob\n+): Promise<{ content: string; tokens: number }[]> => {\n+  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\")\n+  const { RecursiveCharacterTextSplitter } = await import(\n+    \"langchain/text_splitter\"\n+  )\n+  const { encode } = await import(\"gpt-tokenizer\")\n   const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n \n-  const loader = new CSVLoader(csv);\n-  const docs = await loader.load();\n-  let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n \n+  const loader = new CSVLoader(csv)\n+  const docs = await loader.load()\n+  let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n+\n   const splitter = new RecursiveCharacterTextSplitter({\n     chunkSize: CHUNK_SIZE,\n     chunkOverlap: CHUNK_OVERLAP,\n     separators: [\"\\n\\n\"]\n-  });\n-  const splitDocs = await splitter.createDocuments([completeText]);\n+  })\n+  const splitDocs = await splitter.createDocuments([completeText])\n \n-  let chunks: { content: string; tokens: number }[] = [];\n+  let chunks: { content: string; tokens: number }[] = []\n \n   for (let i = 0; i < splitDocs.length; i++) {\n-    const doc = splitDocs[i];\n+    const doc = splitDocs[i]\n \n     chunks.push({\n       content: doc.pageContent,\n       tokens: encode(doc.pageContent).length\n-    });\n+    })\n   }\n \n-  return chunks;\n-};\n+  return chunks\n+}\n"
                },
                {
                    "date": 1718049557883,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,11 +5,10 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\")\n \n-\n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n \n"
                },
                {
                    "date": 1718049574565,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n \n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n"
                },
                {
                    "date": 1718049588414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"../config.ts\");\n \n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n"
                },
                {
                    "date": 1718050549478,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,8 +7,9 @@\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n   const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"../config.ts\");\n \n+\n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n \n"
                },
                {
                    "date": 1718050567042,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,11 +5,10 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"../config.ts\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n \n-\n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n \n"
                },
                {
                    "date": 1718050586888,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,13 +1,13 @@\n export const processCSV = async (\n   csv: Blob\n ): Promise<{ content: string; tokens: number }[]> => {\n-  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\")\n+  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv.ts\")\n   const { RecursiveCharacterTextSplitter } = await import(\n-    \"langchain/text_splitter\"\n+    \"langchain/text_splitter.ts\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\")\n \n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n"
                },
                {
                    "date": 1718050604244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,13 +1,13 @@\n export const processCSV = async (\n   csv: Blob\n ): Promise<{ content: string; tokens: number }[]> => {\n-  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv.ts\")\n+  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\")\n   const { RecursiveCharacterTextSplitter } = await import(\n-    \"langchain/text_splitter.ts\"\n+    \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"./config.ts\")\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"/Users/jeffsperandeo/Projects/chatbot-ui-fed/lib/retrieval/processing/config.ts\");\n \n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n"
                },
                {
                    "date": 1718050611430,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,9 +5,9 @@\n   const { RecursiveCharacterTextSplitter } = await import(\n     \"langchain/text_splitter\"\n   )\n   const { encode } = await import(\"gpt-tokenizer\")\n-  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"/Users/jeffsperandeo/Projects/chatbot-ui-fed/lib/retrieval/processing/config.\");\n+  const { CHUNK_OVERLAP, CHUNK_SIZE } = await import(\"/Users/jeffsperandeo/Projects/chatbot-ui-fed/lib/retrieval/processing/config\");\n \n   const loader = new CSVLoader(csv)\n   const docs = await loader.load()\n   let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\")\n"
                }
            ],
            "date": 1718046405250,
            "name": "Commit-0",
            "content": "export const processCSV = async (csv: Blob): Promise<FileItemChunk[]> => {\n  const { CSVLoader } = await import(\"langchain/document_loaders/fs/csv\");\n  const { RecursiveCharacterTextSplitter } = await import(\"langchain/text_splitter\");\n  import { FileItemChunk } from \"@/types\";\n  import { encode } from \"gpt-tokenizer\";\n  import { CHUNK_OVERLAP, CHUNK_SIZE } from \".\";\n\n  const loader = new CSVLoader(csv);\n  const docs = await loader.load();\n  let completeText = docs.map(doc => doc.pageContent).join(\"\\n\\n\");\n\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: CHUNK_SIZE,\n    chunkOverlap: CHUNK_OVERLAP,\n    separators: [\"\\n\\n\"]\n  });\n  const splitDocs = await splitter.createDocuments([completeText]);\n\n  let chunks: FileItemChunk[] = [];\n\n  for (let i = 0; i < splitDocs.length; i++) {\n    const doc = splitDocs[i];\n\n    chunks.push({\n      content: doc.pageContent,\n      tokens: encode(doc.pageContent).length\n    });\n  }\n\n  return chunks;\n};\n"
        }
    ]
}